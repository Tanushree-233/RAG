{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81cfb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q streamlit faiss-cpu sentence-transformers langchain langchain-community huggingface_hub pyngrok kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eaa8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import files\n",
    "files.upload()  # ðŸ‘‰ Upload your kaggle.json here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2684cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d sonalisingh1411/loan-approval-prediction\n",
    "!unzip -o loan-approval-prediction.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"utils.py\", \"w\") as f:\n",
    "    f.write(\"\"\"import pandas as pd\n",
    "\n",
    "def load_csv(file_obj):\n",
    "    df = pd.read_csv(file_obj)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def rows_to_documents(df):\n",
    "    return df.apply(lambda row: \" | \".join(f\"{col}: {val}\" for col, val in row.items()), axis=1).tolist()\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a044df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"retriever.py\", \"w\") as f:\n",
    "    f.write(\"\"\"import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "\n",
    "    def fit(self, docs):\n",
    "        self.documents = docs\n",
    "        embeddings = self.model.encode(docs)\n",
    "        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings))\n",
    "\n",
    "    def get_relevant_docs(self, query, k=5):\n",
    "        query_emb = self.model.encode([query])\n",
    "        _, I = self.index.search(np.array(query_emb), k)\n",
    "        return [self.documents[i] for i in I[0]]\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5486465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"qa_chain.py\", \"w\") as f:\n",
    "    f.write(\"\"\"from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "class QABot:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        self.llm = HuggingFaceHub(\n",
    "            repo_id=\"google/flan-t5-base\",\n",
    "            model_kwargs={\"temperature\": 0.1, \"max_length\": 512}\n",
    "        )\n",
    "        self.chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=self.llm,\n",
    "            retriever=self.retriever,\n",
    "            memory=self.memory\n",
    "        )\n",
    "\n",
    "    def ask(self, query):\n",
    "        return self.chain.run(query)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2817c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Set Hugging Face API token BEFORE importing QABot\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_your_token_here\"  # Replace with your actual token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4aae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(\"\"\"import streamlit as st\n",
    "from utils import load_csv, rows_to_documents\n",
    "from retriever import Retriever\n",
    "from qa_chain import QABot\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "st.title(\"ðŸ“Š Loan Approval Chatbot (FREE MODE)\")\n",
    "\n",
    "uploaded = st.sidebar.file_uploader(\"Upload Loan CSV\", type=\"csv\")\n",
    "\n",
    "if \"bot\" not in st.session_state:\n",
    "    st.session_state.bot = None\n",
    "\n",
    "if uploaded:\n",
    "    df = load_csv(uploaded)\n",
    "    docs = rows_to_documents(df)\n",
    "    retr = Retriever()\n",
    "    retr.fit(docs)\n",
    "    st.session_state.bot = QABot(retriever=retr)\n",
    "    st.success(\"âœ… Chatbot is readyâ€”ask any question!\")\n",
    "\n",
    "if st.session_state.bot:\n",
    "    if \"history\" not in st.session_state:\n",
    "        st.session_state.history = []\n",
    "    query = st.text_input(\"Ask something:\")\n",
    "    if query:\n",
    "        ans = st.session_state.bot.ask(query)\n",
    "        st.session_state.history.append((query, ans))\n",
    "\n",
    "    for q, a in reversed(st.session_state.history):\n",
    "        st.markdown(f\"**Q:** {q}\")\n",
    "        st.markdown(f\"**A:** {a}\")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9480f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pkill -f streamlit\n",
    "!streamlit run app.py &> logs.txt &\n",
    "from pyngrok import ngrok\n",
    "print(\"ðŸ”— Public link:\", ngrok.connect(8501))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f812e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!tail -n 50 logs.txt\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
